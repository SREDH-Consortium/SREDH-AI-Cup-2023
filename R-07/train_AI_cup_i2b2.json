{
    "post_process":"argmax",
    "threshold": null,
    "model_name_or_path":"roberta-large",
    "task_name":"ner",
    "notation":"BILOU",
    "ner_types":["DOCTOR", "DATE", "IDNUM", "MEDICALRECORD", "PATIENT", "HOSPITAL", "TIME", "DEPARTMENT", "CITY", "ZIP", "STREET", "STATE", "ORGANIZATION", "AGE", "DURATION", "SET", "PHONE", "LOCATION-OTHER", "COUNTRY", "URL", "ROOM", "USERNAME", "PROFESSION", "FAX", "DEVICE", "EMAIL", "BIOID", "HEALTHPLAN"],
    "train_file":["First_Phase_Text_Dataset_addi2b2/formal_chunk_size_32/ner_datasets/train.jsonl"],
    "validation_file":["First_Phase_Text_Dataset_addi2b2/formal_chunk_size_32/ner_datasets/validation.jsonl"],
    "test_file":null,
    "truncation":true,
    "max_length":250,
    "label_all_tokens":false,
    "preprocessing_num_workers":6,
    "return_entity_level_metrics":true,
    "text_column_name":"tokens",
    "label_column_name":"labels",
    "model_eval_script":"/home/ting/code/ehr_deidentification/src/robust_deid/sequence_tagging/evaluation/note_evaluation/note_evaluation.py",
    "validation_spans_file":"First_Phase_Text_Dataset_addi2b2/formal_chunk_size_32/validation_spans.jsonl",
    "evaluation_mode":"strict",
    "ner_type_maps":[
        ["PHI", "PHI","PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI","PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI", "PHI"],
        ["PHI+", "PHI+","PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+","PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+", "PHI+"]
   ],
    "output_dir":"chpt_addi2b2/formal_roberta-large_allsplit/output",
    "logging_dir":"chpt_addi2b2/formal_roberta-large_allsplit/log",
    "overwrite_output_dir":true,
    "do_train":true,
    "do_eval":true,
    "do_predict":false,
    "report_to":["tensorboard"],
    "per_device_train_batch_size":2,
    "per_device_eval_batch_size":2,
    "gradient_accumulation_steps":8,
    "weight_decay":0.01,
    "learning_rate":5e-5,
    "num_train_epochs":10,
    "adam_epsilon":1e-06,
    "warmup_ratio":0.3,
    "logging_steps":5,
    "evaluation_strategy":"no",
    "save_strategy":"epoch"
   }