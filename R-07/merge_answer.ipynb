{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併 predict 結果\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "target = pd.read_csv('/home/ting/code/ehr_deidentification/formal_test_addi2b2/formal_all_split_lower_continue/predict_67210_epoch10_predict_regex_entity_imp.txt', keep_default_na=False, na_values=['NaN', 'null'],names=[\"file_id\",\"label\",\"start\",\"end\",\"entity\",\"regular\"], delimiter='\\t',)\n",
    "pivot = pd.read_csv('formal_test_addi2b2/formal_chunk_size_32/predict_66430_epoch10_predict_regex_entity_imp.txt',keep_default_na=False, na_values=['NaN', 'null'], names=[\"file_id\",\"label\",\"start\",\"end\",\"entity\",\"regular\"], delimiter='\\t',)\n",
    "\n",
    "pivot_entity_group = pivot.groupby('file_id')\n",
    "target_entity_group = target.groupby('file_id')\n",
    "\n",
    "\n",
    "finish_data = pd.DataFrame(columns=[\"file_id\",\"label\",\"start\",\"end\",\"entity\",\"regular\"])\n",
    "\n",
    "for file_name in pivot_entity_group.groups.keys():\n",
    "    # file_name='1032'\n",
    "    specific_pivot_group = pivot_entity_group.get_group(file_name)\n",
    "    specific_target_group = target_entity_group.get_group(file_name)\n",
    "    \n",
    "    same_start = specific_target_group.merge(specific_pivot_group, on=[\"start\"], how='left')\n",
    "    for idx, item in same_start.iterrows():\n",
    "        # print(item.head())\n",
    "        # print(item[['file_id_x','label_x','start','end_x','entity_x','regular_x']])\n",
    "        # print(type(item))\n",
    "        # print(item['end_y'],float('NaN'))\n",
    "        if item['end_x'] != item['end_y'] and not np.isnan(item['end_y']):\n",
    "            if len(str(item['entity_x']))>len(str(item['entity_y'])):\n",
    "                new_item = item[['file_id_x','label_x','start','end_x','entity_x','regular_x']]\n",
    "                new_item.index = [\"file_id\",\"label\",\"start\",\"end\",\"entity\",\"regular\"]\n",
    "                new_item['end'] = int(new_item['end'])\n",
    "                # print(type(new_item['end']),new_item['end'])\n",
    "                finish_data = finish_data.append(new_item)\n",
    "            else:\n",
    "                new_item = item[['file_id_y','label_y','start','end_y','entity_y','regular_y']]\n",
    "                new_item.index = [\"file_id\",\"label\",\"start\",\"end\",\"entity\",\"regular\"]\n",
    "                new_item['end'] = int(new_item['end'])\n",
    "                # print(type(new_item['end']),new_item['end'])\n",
    "                finish_data = finish_data.append(new_item)\n",
    "        else:\n",
    "            new_item = item[['file_id_x','label_x','start','end_x','entity_x','regular_x']]\n",
    "            new_item.index = [\"file_id\",\"label\",\"start\",\"end\",\"entity\",\"regular\"]\n",
    "            new_item['end'] = int(new_item['end'])\n",
    "            # print(type(new_item['end']),new_item['end'])\n",
    "            finish_data = finish_data.append(new_item)\n",
    "        # break\n",
    "    # break\n",
    "\n",
    "finish_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# finish_data\n",
    "finish_data.to_csv('formal_test_addi2b2/predict_67210_test_regex_imp_(combine_i2b2(no_all_split))_new.txt', sep='\\t', index=False,header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併 chatgpt 正規化日期以及 not add_i2b2 資料集的部分預測 label\n",
    "import pandas as pd\n",
    "import re\n",
    "answer = pd.read_csv('formal_test_addi2b2/predict_67210_test_regex_imp_(combine_i2b2(no_all_split))_new.txt', names=[\"file_id\", \"label\", \"start\", \"end\", \"entity\",\"regular\"],keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t',)\n",
    "n_t_p = pd.read_csv('/home/ting/code/ehr_deidentification/Need_chatgpt_date_fix.txt', names=[\"file_id\", \"label\", \"start\", \"end\", \"entity\",\"regular\"],keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t',)\n",
    "\n",
    "# answer = answer[answer['label'] == 'DATE']\n",
    "need_gpt_date = pd.DataFrame()\n",
    "target_file = answer[answer['entity'].apply(lambda x: bool(re.findall(r\"^[Nn][Oo][Ww]$\", x)))]\n",
    "need_gpt_date = pd.concat([need_gpt_date,target_file])\n",
    "target_file = answer[answer['entity'].apply(lambda x: bool(re.findall(r\"^[Tt][Oo][Dd][Aa][Yy]$\", x)))]\n",
    "need_gpt_date = pd.concat([need_gpt_date,target_file])\n",
    "target_file = answer[answer['entity'].apply(lambda x: bool(re.findall(r\"^[Pp][Rr][Ee][Vv][Ii][Oo][Uu][Ss]$\", x)))]\n",
    "need_gpt_date = pd.concat([need_gpt_date,target_file])\n",
    "\n",
    "answer.drop(need_gpt_date.index.to_list(),inplace=True)\n",
    "answer = pd.concat([answer,n_t_p])\n",
    "answer = answer.sort_values(by=['file_id',\"start\"])\n",
    "answer = answer.reset_index(drop=True)\n",
    "\n",
    "# not add_i2b2 資料集的部分預測 label\n",
    "pivot = pd.read_csv('formal_test/formal_chunk_size_32/predict_41348_test_regex_entity_imp.txt', keep_default_na=False, na_values=['NaN', 'null'],names=[\"file_id\",\"label\",\"start\",\"end\",\"entity\",\"regular\"], delimiter='\\t',)\n",
    "\n",
    "pivot_part1 = pivot[pivot['label']==\"DURATION\"]\n",
    "pivot_part2 = pivot[pivot['label']==\"PHONE\"]\n",
    "pivot_part3 = pivot[pivot['label']==\"TIME\"]\n",
    "pivot = pd.concat([pivot_part1,pivot_part2,pivot_part3])\n",
    "answer = answer[answer['label']!=\"DURATION\"]\n",
    "answer = answer[answer['label']!=\"PHONE\"]\n",
    "answer = answer[answer['label']!=\"TIME\"]\n",
    "answer = answer.reset_index(drop=True)\n",
    "entity_group = answer.groupby('file_id')\n",
    "# finish_data = pd.DataFrame(columns=[\"file_id\",\"label\",\"start\",\"end\",\"entity\",\"regular\"])\n",
    "for idx,item in pivot.iterrows():\n",
    "    # entity = location_other.group()\n",
    "    start = int(item['start'])\n",
    "    end = int(item['end'])\n",
    "\n",
    "    answer.drop(entity_group.get_group(item['file_id']).query('start>=@start and end<=@end').index.tolist(),inplace=True)\n",
    "    imp_entity = pd.DataFrame({\n",
    "        \"file_id\":item['file_id'],\n",
    "        \"label\":item['label'],\n",
    "        \"start\":start,\n",
    "        \"end\":end,\n",
    "        \"entity\":item['entity'],\n",
    "        \"regular\":item['regular']\n",
    "    }, index=[0])\n",
    "    # print(imp_entity)\n",
    "    answer = answer.append(imp_entity, ignore_index=True)\n",
    "answer = answer.sort_values(by=['file_id',\"start\"])\n",
    "answer = answer.reset_index(drop=True)\n",
    "# answer.to_csv(\"formal_test_addi2b2/formal_all_split_lower_continue/predict_67210_epoch10_predict_regex_(combine_i2b2(no_all_split))_ACE.txt\", sep='\\t', index=False,header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#刪除merge後重疊資料 可多跑幾次\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# answer = pd.read_csv('formal_test_addi2b2/predict_67210_test_regex_imp_(combine_i2b2(no_all_split))_new_ACE.txt', keep_default_na=False, na_values=['NaN', 'null'],names=[\"file_id\",\"label\",\"start\",\"end\",\"entity\",\"regular\"], delimiter='\\t',)\n",
    "answer_entity_group = answer.groupby('file_id')\n",
    "\n",
    "finish_data = pd.DataFrame(columns=[\"file_id\",\"label\",\"start\",\"end\",\"entity\",\"regular\"])\n",
    "for file_name in answer_entity_group.groups.keys():\n",
    "    group = answer_entity_group.get_group(file_name)\n",
    "    for i,(idx, item) in enumerate(group.iterrows()): \n",
    "        if i==0:\n",
    "            finish_data = finish_data.append(item, ignore_index=True)\n",
    "            pre_start=item['start']\n",
    "            pre_end=item['end']\n",
    "            pre_entity = item['label']\n",
    "            continue\n",
    "        elif item['start']>=pre_start and item['start']<pre_end:\n",
    "            # print(item)                \n",
    "            pre_start=item['start']\n",
    "            pre_end=item['end']\n",
    "            pre_entity = item['label']\n",
    "        else:\n",
    "            finish_data = finish_data.append(item, ignore_index=True)\n",
    "            # print(item)\n",
    "            pre_start=item['start']\n",
    "            pre_end=item['end']\n",
    "            pre_entity = item['label']\n",
    "    # break\n",
    "finish_data.to_csv(\"formal_test_addi2b2/predict_67210_test_regex_imp_(combine_i2b2(no_all_split))_new_ACE.txt\", sep='\\t', index=False,header=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('ehr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5743b65d27269470b70e06a455afa68522af6fbc6633ff9431882b15b23769fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
